{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os, time, itertools, imageio, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-7c1e800a0c0b>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From I:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From I:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From I:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From I:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From I:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "train_epoch = 50\n",
    "nb_classes = 10\n",
    "\n",
    "\n",
    "# load MNIST\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_set = (mnist.train.images) \n",
    "#add normalization\n",
    "train_label = mnist.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(label = 0):\n",
    "    _z = np.random.rand(1,100)\n",
    "    targets = np.array([[label]]).reshape(-1)\n",
    "    one_hot_targets = np.eye(nb_classes)[targets]\n",
    "    pop = sess.run(G_z, {z: _z, y: one_hot_targets, isTrain: False})\n",
    "    return pop.reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeGif(filename = 'testing.gif', fps = 5):\n",
    "    data = []\n",
    "    for i in range(10):\n",
    "        x = eval(i)\n",
    "        data.append(x)\n",
    "    imageio.mimsave(filename, data, fps=fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(x, y, isTrain=True, reuse=False):\n",
    "    with tf.variable_scope('gen', reuse=reuse):\n",
    "        w_init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        cat1 = tf.concat([x, y], 1)\n",
    "\n",
    "        dense1 = tf.layers.dense(cat1, 128, kernel_initializer=w_init)\n",
    "        relu1 = tf.nn.relu(dense1)\n",
    "\n",
    "        dense2 = tf.layers.dense(relu1, 784, kernel_initializer=w_init)\n",
    "        o = tf.nn.tanh(dense2)\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x, y, isTrain=True, reuse=False):\n",
    "    with tf.variable_scope('dis', reuse=reuse):\n",
    "        w_init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        cat1 = tf.concat([x, y], 1)\n",
    "\n",
    "        dense1 = tf.layers.dense(cat1, 128, kernel_initializer=w_init)\n",
    "        lrelu1 = tf.nn.relu(dense1)\n",
    "\n",
    "        dense2 = tf.layers.dense(lrelu1, 1, kernel_initializer=w_init)\n",
    "        o = tf.nn.sigmoid(dense2)\n",
    "\n",
    "        return o, dense2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = np.eye(10)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "z = tf.placeholder(tf.float32, shape=(None, 100))\n",
    "isTrain = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "G_z = generator(z, y, isTrain)\n",
    "\n",
    "D_real, D_real_logits = discriminator(x, y, isTrain)\n",
    "D_fake, D_fake_logits = discriminator(G_z, y, isTrain, reuse=True)\n",
    "\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones([batch_size, 1])))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros([batch_size, 1])))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones([batch_size, 1])))\n",
    "\n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('dis')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('gen')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    D_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(D_loss, var_list=D_vars)\n",
    "    G_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(G_loss, var_list=G_vars)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50] - loss_d: 1.280, loss_g: 0.882\n",
      "[2/50] - loss_d: 1.035, loss_g: 0.993\n",
      "[3/50] - loss_d: 1.024, loss_g: 1.108\n",
      "[4/50] - loss_d: 1.042, loss_g: 1.130\n",
      "[5/50] - loss_d: 1.007, loss_g: 1.204\n",
      "[6/50] - loss_d: 0.935, loss_g: 1.286\n",
      "[7/50] - loss_d: 0.896, loss_g: 1.330\n",
      "[8/50] - loss_d: 0.951, loss_g: 1.201\n",
      "[9/50] - loss_d: 1.053, loss_g: 1.097\n",
      "[10/50] - loss_d: 1.068, loss_g: 1.116\n",
      "[11/50] - loss_d: 1.078, loss_g: 1.111\n",
      "[12/50] - loss_d: 1.063, loss_g: 1.107\n",
      "[13/50] - loss_d: 1.070, loss_g: 1.109\n",
      "[14/50] - loss_d: 1.081, loss_g: 1.126\n",
      "[15/50] - loss_d: 1.030, loss_g: 1.105\n",
      "[16/50] - loss_d: 0.995, loss_g: 1.120\n",
      "[17/50] - loss_d: 0.998, loss_g: 1.156\n",
      "[18/50] - loss_d: 0.983, loss_g: 1.208\n",
      "[19/50] - loss_d: 0.955, loss_g: 1.216\n",
      "[20/50] - loss_d: 0.935, loss_g: 1.271\n",
      "[21/50] - loss_d: 0.911, loss_g: 1.303\n",
      "[22/50] - loss_d: 0.901, loss_g: 1.287\n",
      "[23/50] - loss_d: 0.899, loss_g: 1.280\n",
      "[24/50] - loss_d: 0.894, loss_g: 1.283\n",
      "[25/50] - loss_d: 0.881, loss_g: 1.350\n",
      "[26/50] - loss_d: 0.878, loss_g: 1.364\n",
      "[27/50] - loss_d: 0.887, loss_g: 1.360\n",
      "[28/50] - loss_d: 0.878, loss_g: 1.393\n",
      "[29/50] - loss_d: 0.895, loss_g: 1.401\n",
      "[30/50] - loss_d: 0.884, loss_g: 1.394\n",
      "[31/50] - loss_d: 0.897, loss_g: 1.370\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(train_epoch):\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "    epoch_start_time = time.time()\n",
    "    for iter in range(len(train_set) // batch_size):\n",
    "        # update discriminator\n",
    "        x_ = train_set[iter * batch_size:(iter + 1) * batch_size]\n",
    "        y_ = train_label[iter * batch_size:(iter + 1) * batch_size]\n",
    "\n",
    "        z_ = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "        loss_d_, _ = sess.run([D_loss, D_optim], {x: x_, y: y_, z: z_, isTrain: True})\n",
    "        D_losses.append(loss_d_)\n",
    "\n",
    "        # update generator\n",
    "        z_ = np.random.normal(0, 1, (batch_size, 100))\n",
    "        y_ = np.random.randint(0, 9, (batch_size, 1))\n",
    "        y_ = onehot[y_.astype(np.int32)].squeeze()\n",
    "        loss_g_, _ = sess.run([G_loss, G_optim], {z: z_, x: x_, y: y_, isTrain: True})\n",
    "        G_losses.append(loss_g_)\n",
    "\n",
    "    print('[%d/%d] - loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, np.mean(D_losses), np.mean(G_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeGif('Results.gif')\n",
    "plt.imsave('Result_1.png', eval(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
